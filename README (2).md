# 📝 Text Summarization Using Pre-trained Models

This project is part of my **Elevvo Internship** and focuses on building an **abstractive text summarization system** using state-of-the-art transformer models.

---

## 📌 Project Overview
The goal of this project is to generate concise and meaningful summaries from long news articles using pre-trained models. I used the **GEM/XSum dataset** for training and evaluation, which is a standard benchmark for abstractive summarization tasks.

---

## ⚙️ Tools & Technologies
- **Python**
- **Hugging Face Transformers**
- **Datasets (GEM/XSum)**
- **PyTorch**
- **Matplotlib & WordCloud** (for visualization)

---

## 🚀 What I Did
- Loaded and preprocessed the **GEM/XSum dataset** for summarization tasks.
- Implemented summarization pipelines using **BART** and **T5** models.
- Allowed custom text input to generate summaries dynamically.
- Visualized text distributions and word clouds for better insights.
- Compared performance of different models on validation samples.

---

## 📊 Example

**Original Text:**
> "The European Union has announced new policies aimed at reducing carbon emissions by 55% before 2030. The plan includes measures on renewable energy, electric vehicles, and carbon taxes."

**Generated Summary (BART):**
> "EU announces policies to cut carbon emissions by 55% before 2030."

---

## 📈 Results
- **BART-large-xsum** produced fluent and coherent summaries.
- **T5-base** was effective but sometimes less concise compared to BART.
- WordCloud visualizations showed the most frequent terms in documents and summaries.

---

## 🔗 Repository
Full code and notebook available here: [GitHub Repository](https://github.com/Hebatuallah1/Text-Summarization)

---

## 🙏 Acknowledgements
Thanks to the **Elevvo Team** for guiding me through this project and helping me gain hands-on experience with **NLP and Transformers**.

---

### Hashtags
#Elevvo #Internship #AI #NLP #Transformers #Summarization #MachineLearning #DeepLearning
